{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Retraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import tqdm\n",
    "from math import floor\n",
    "\n",
    "# wandb - hyperparameter sweep and Train monitoring\n",
    "import wandb\n",
    "#torch - computing and machine learning libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "# seisbench\n",
    "import seisbench.models as sbm\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#gp\n",
    "import gpytorch\n",
    "\n",
    "# seisynth\n",
    "from utils.common import load_dataset_and_labels, load_pretrained_model\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Possible values\n",
    "DATASETS_ORIGINS = ['ethz', 'geofon']\n",
    "SBM_CLASSES= [sbm.PhaseNet, sbm.EQTransformer]\n",
    "MODEL_TO_NUM_SAMPLES = {sbm.EQTransformer:6000, sbm.PhaseNet: 3001}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset_origin = 'geofon'\n",
    "assert dataset_origin in DATASETS_ORIGINS, f'Expected dataset one of {DATASETS_ORIGINS}. Got {dataset_origin}.'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "seisbench.models.phasenet.PhaseNet"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SBM_CLASS= sbm.PhaseNet\n",
    "assert SBM_CLASS in SBM_CLASSES\n",
    "SBM_CLASS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "3001"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES=MODEL_TO_NUM_SAMPLES[SBM_CLASS]\n",
    "NUM_SAMPLES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "NUM_SHIFTS=6\n",
    "SAMPLE_RATE=100\n",
    "LARGE_ERROR_THRESHOLD_SECONDS=1\n",
    "LARGE_ERROR_THRESHOLD_SAMPLES=LARGE_ERROR_THRESHOLD_SECONDS*SAMPLE_RATE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# A list of indices of the original dataset used for noising - see 'create_noisy_dataset.ipynb'\n",
    "# Do not use these indices when evaluating the retrained\n",
    "INDICES_USED_WHILE_NOISING = list(range(6000))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYNTHESIZED_SNR_LIST= list(range(1,11))\n",
    "SYNTHESIZED_SNR_LIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def assert_path_exists(path_str: str, name: str=''):\n",
    "    assert os.path.exists(path_str), f'{name} {path_str} does not exist'\n",
    "\n",
    "@torch.no_grad()\n",
    "def standardize(trace: torch.tensor):\n",
    "    m = trace.mean(dim=-1, keepdim=True).unsqueeze(dim=0)\n",
    "    std = trace.std(dim=-1, keepdim=True).unsqueeze(dim=0)\n",
    "    trace = trace.unsqueeze(dim=0) if trace.dim() == 1 else trace\n",
    "    standardized = torch.stack([(trace[ch] - m[0, ch]) / std[0, ch] for ch in range(trace.shape[0])], dim=0)\n",
    "    assert standardized.shape == trace.shape, f'Standardization should not change shape. Got {standardized.shape}'\n",
    "    return standardized"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH=f'/home/moshe/datasets/GFZ/noisy_datasets/{dataset_origin}_trainset_{NUM_SAMPLES}_sample_joachim_noises_energy_ratio_snr/'\n",
    "assert_path_exists(path_str=DATASET_PATH, name='DATASET_PATH')\n",
    "DATASET_PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_1',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_2',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_3',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_4',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_5',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_6',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_7',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_8',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_9',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOISY_DATA_PATH_LIST = [os.path.join(DATASET_PATH, f'noisy_dataset_snr_{synthesized_snr}') for synthesized_snr in SYNTHESIZED_SNR_LIST]\n",
    "for p in NOISY_DATA_PATH_LIST:\n",
    "    assert_path_exists(path_str=p)\n",
    "NOISY_DATA_PATH_LIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Pretrained Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the model with the pretrained weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with <class 'seisbench.models.phasenet.PhaseNet'> on GEOFON\n",
      "Load <class 'seisbench.models.phasenet.PhaseNet'> pretrained weights\n",
      "<class 'seisbench.models.phasenet.PhaseNet'> pretrained keys ['ethz', 'geofon', 'instance', 'iquique', 'lendb', 'neic', 'original', 'scedc', 'stead']\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = load_pretrained_model(model_class=SBM_CLASS, dataset_trained_on=dataset_origin)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save a copy for retraining. One model will be trained and the other one will keep the current weights for benchmarking on specific examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with <class 'seisbench.models.phasenet.PhaseNet'> on GEOFON\n",
      "Load <class 'seisbench.models.phasenet.PhaseNet'> pretrained weights\n",
      "<class 'seisbench.models.phasenet.PhaseNet'> pretrained keys ['ethz', 'geofon', 'instance', 'iquique', 'lendb', 'neic', 'original', 'scedc', 'stead']\n"
     ]
    }
   ],
   "source": [
    "# reloading because I cannot torch clone. Seisbench models are not nn.Module :(\n",
    "retraining_model = load_pretrained_model(model_class=SBM_CLASS, dataset_trained_on=dataset_origin)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "PhaseNet(\n  (inc): Conv1d(3, 8, kernel_size=(7,), stride=(1,), padding=same)\n  (in_bn): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  (down_branch): ModuleList(\n    (0): ModuleList(\n      (0): Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (1): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(8, 8, kernel_size=(7,), stride=(4,), padding=(3,), bias=False)\n      (3): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): ModuleList(\n      (0): Conv1d(8, 16, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (1): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(16, 16, kernel_size=(7,), stride=(4,), bias=False)\n      (3): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): ModuleList(\n      (0): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (1): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(32, 32, kernel_size=(7,), stride=(4,), bias=False)\n      (3): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): ModuleList(\n      (0): Conv1d(32, 64, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(64, 64, kernel_size=(7,), stride=(4,), bias=False)\n      (3): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): ModuleList(\n      (0): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (1): BatchNorm1d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): None\n      (3): None\n    )\n  )\n  (up_branch): ModuleList(\n    (0): ModuleList(\n      (0): ConvTranspose1d(128, 64, kernel_size=(7,), stride=(4,), bias=False)\n      (1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(128, 64, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (3): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): ModuleList(\n      (0): ConvTranspose1d(64, 32, kernel_size=(7,), stride=(4,), bias=False)\n      (1): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (3): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): ModuleList(\n      (0): ConvTranspose1d(32, 16, kernel_size=(7,), stride=(4,), bias=False)\n      (1): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (3): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): ModuleList(\n      (0): ConvTranspose1d(16, 8, kernel_size=(7,), stride=(4,), bias=False)\n      (1): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Conv1d(16, 8, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n      (3): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (out): Conv1d(8, 3, kernel_size=(1,), stride=(1,), padding=same)\n  (softmax): Softmax(dim=1)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.eval()\n",
    "retraining_model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Datasets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have 4 datasets:\n",
    "1. original_dataset - ETHZ/GEOFON original traces filtered to have high estimated SNR - more than 20dB\n",
    "2. le_original_dataset - A subset of the original_dataset (high SNR traces) that the pretrained model had a large picking error.\n",
    "3. noised_dataset - Traces taken from the original dataset and merged with noise traces such that the resulting trace is a 10 dB SNR trace.\n",
    "4. le_noised_dataset -  A subset of the noised_dataset that the pretrained model had a large picking error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Noisy Traces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load synthetic noisy traces with various SNR levels and mix them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_1/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_2/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_3/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_4/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_5/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_6/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_7/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_8/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_9/traces.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10/traces.pt']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOISY_DATA_PATH_TRACES_LIST = [os.path.join(ndpl, 'traces.pt') for ndpl in NOISY_DATA_PATH_LIST]\n",
    "for p in NOISY_DATA_PATH_TRACES_LIST:\n",
    "    assert_path_exists(path_str=p)\n",
    "NOISY_DATA_PATH_TRACES_LIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_1/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_2/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_3/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_4/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_5/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_6/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_7/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_8/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_9/labels.pt',\n '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10/labels.pt']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOISY_DATA_PATH_LABELS_LIST = [os.path.join(ndpl, 'labels.pt') for ndpl in NOISY_DATA_PATH_LIST]\n",
    "for p in NOISY_DATA_PATH_LABELS_LIST:\n",
    "    assert_path_exists(path_str=p)\n",
    "NOISY_DATA_PATH_LABELS_LIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# noised_dataset_path= os.path.join(NOISY_DATA_PATH, 'traces.pt')\n",
    "# assert_path_exists(path_str=noised_dataset_path)\n",
    "# noised_labels_path= os.path.join(NOISY_DATA_PATH, 'labels.pt')\n",
    "# assert_path_exists(path_str=noised_labels_path)\n",
    "# noised_dataset_path, noised_labels_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces_list 1 labels_list 1\n",
      "traces_list 2 labels_list 2\n",
      "traces_list 3 labels_list 3\n",
      "traces_list 4 labels_list 4\n",
      "traces_list 5 labels_list 5\n",
      "traces_list 6 labels_list 6\n",
      "traces_list 7 labels_list 7\n",
      "traces_list 8 labels_list 8\n",
      "traces_list 9 labels_list 9\n",
      "traces_list 10 labels_list 10\n",
      "traces shape torch.Size([5000, 3, 3001]) labels_list torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "def load_dataset_from_tensors(traces_path_list: list[torch.tensor], labels_path_list: list[torch.tensor], indices_to_use: list[int]=[]):\n",
    "    traces_list, labels_list = [], []\n",
    "    for tp, lp in zip(traces_path_list, labels_path_list):\n",
    "        traces,labels = load_dataset_and_labels(dataset_path=tp, labels_path=lp)\n",
    "        if indices_to_use:\n",
    "            traces, labels = traces[indices_to_use], labels[indices_to_use]\n",
    "        traces_list.append(traces)\n",
    "        labels_list.append(labels.unsqueeze(dim=1))\n",
    "        print(f'traces_list {len(traces_list)} labels_list {len(labels_list)}')\n",
    "\n",
    "\n",
    "    traces = torch.vstack(traces_list)\n",
    "    labels = torch.vstack(labels_list).squeeze()\n",
    "    print(f'traces shape {traces.shape} labels_list {labels.shape}')\n",
    "    return traces, labels\n",
    "\n",
    "noised_dataset, noised_labels = load_dataset_from_tensors(traces_path_list=NOISY_DATA_PATH_TRACES_LIST, labels_path_list=NOISY_DATA_PATH_LABELS_LIST, indices_to_use=list(range(500)))\n",
    "\n",
    "noised_dataset_size = noised_dataset.shape[0]\n",
    "\n",
    "# noised_dataset, noised_labels = load_dataset_and_labels(dataset_path=noised_dataset_path, labels_path=noised_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 traces\n"
     ]
    }
   ],
   "source": [
    "print(f'Loaded {noised_dataset_size} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Original Data - Various SNR Levels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following are traces from the original dataset with SNR of all levels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(['/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/traces_geofon_bounds_-20_0.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/traces_geofon_bounds_0_2.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/traces_geofon_bounds_2_5.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/traces_geofon_bounds_5_10.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/traces_geofon_bounds_10_20.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/traces_geofon_bounds_20_100.pt'],\n ['/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/labels_geofon_bounds_-20_0.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/labels_geofon_bounds_0_2.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/labels_geofon_bounds_2_5.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/labels_geofon_bounds_5_10.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/labels_geofon_bounds_10_20.pt',\n  '/home/moshe/datasets/GFZ/noisy_datasets/geofon_trainset_3001_sample_joachim_noises_energy_ratio_snr/labels_geofon_bounds_20_100.pt'])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of SNR bin edges to which the traces seperated\n",
    "bounds = [-20, 0, 2, 5, 10, 20, 100]\n",
    "# j=5\n",
    "t_list, l_list = [], []\n",
    "for j in range(len(bounds)-1):\n",
    "    suffix = f'_{dataset_origin}_bounds_{bounds[j]}_{bounds[j+1]}.pt'\n",
    "    t = os.path.join(DATASET_PATH, 'traces' + suffix)\n",
    "    assert_path_exists(path_str=t)\n",
    "    t_list.append(t)\n",
    "    l = os.path.join(DATASET_PATH, 'labels' + suffix)\n",
    "    assert_path_exists(path_str=l)\n",
    "    l_list.append(l)\n",
    "t_list, l_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traces_list 1 labels_list 1\n",
      "traces_list 2 labels_list 2\n",
      "traces_list 3 labels_list 3\n",
      "traces_list 4 labels_list 4\n",
      "traces_list 5 labels_list 5\n",
      "traces_list 6 labels_list 6\n",
      "traces shape torch.Size([6000, 3, 3001]) labels_list torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "original_snr_binned_dataset, original_snr_binned_labels = load_dataset_from_tensors(traces_path_list=t_list, labels_path_list=l_list, indices_to_use=list(range(1000)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6000 traces\n"
     ]
    }
   ],
   "source": [
    "print(f'Loaded {original_snr_binned_dataset.shape[0]} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make the real data set size equal to the synthetic one and leave the rest for test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([5000, 3, 3001]),\n torch.Size([5000]),\n torch.Size([1000, 3, 3001]),\n torch.Size([1000]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_snr_binned_dataset, original_snr_binned_labels, original_snr_binned_dataset_left_for_test, original_snr_binned_labels_left_for_test = original_snr_binned_dataset[:noised_dataset_size], original_snr_binned_labels[:noised_dataset_size], original_snr_binned_dataset[noised_dataset_size:], original_snr_binned_labels[noised_dataset_size:]\n",
    "\n",
    "\n",
    "original_snr_binned_dataset.shape, original_snr_binned_labels.shape, original_snr_binned_dataset_left_for_test.shape, original_snr_binned_labels_left_for_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mix The Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10000, 3, 3001]), torch.Size([10000]))"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_dataset = torch.vstack((original_snr_binned_dataset, noised_dataset))\n",
    "mixed_labels = torch.vstack((original_snr_binned_labels.unsqueeze(dim=1), noised_labels.unsqueeze(dim=1))).squeeze()\n",
    "mixed_dataset.shape, mixed_labels.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# le_noised_dataset_path = os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_dataset.pt')\n",
    "# assert_path_exists(path_str=le_noised_dataset_path)\n",
    "# le_noised_labels_path = os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_labels.pt')\n",
    "# assert_path_exists(path_str=le_noised_labels_path)\n",
    "# le_noised_dataset_path, le_noised_labels_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# le_noised_dataset, le_noised_labels = load_dataset_and_labels(dataset_path=le_noised_dataset_path, labels_path=le_noised_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# print(f'Loaded {le_noised_dataset.shape[0]} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrange Train\\Validation\\Test Sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# train_dataset_inds, val_dataset_inds, test_dataset_inds = random_split(range(mixed_dataset.shape[0]), [0.85,0.1,0.05], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# train_dataset_inds, val_dataset_inds, test_dataset_inds = random_split(range(original_snr_binned_dataset.shape[0]), [0.85,0.1,0.05], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataset_inds, val_dataset_inds, test_dataset_inds = random_split(range(noised_dataset.shape[0]), [0.85,0.1,0.05], generator=torch.Generator().manual_seed(42))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# train_dataset, val_dataset, test_dataset = original_snr_binned_dataset[train_dataset_inds], original_snr_binned_dataset[val_dataset_inds], original_snr_binned_dataset[test_dataset_inds]\n",
    "# train_labels, val_labels, test_labels = original_snr_binned_labels[train_dataset_inds], original_snr_binned_labels[val_dataset_inds], original_snr_binned_labels[test_dataset_inds]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = mixed_dataset[train_dataset_inds], mixed_dataset[val_dataset_inds], mixed_dataset[test_dataset_inds]\n",
    "train_labels, val_labels, test_labels = mixed_labels[train_dataset_inds], mixed_labels[val_dataset_inds], mixed_labels[test_dataset_inds]\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = noised_dataset[train_dataset_inds], noised_dataset[val_dataset_inds], noised_dataset[test_dataset_inds]\n",
    "# train_labels, val_labels, test_labels = noised_labels[train_dataset_inds], noised_labels[val_dataset_inds], noised_labels[test_dataset_inds]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train set with 3 traces, validation set with 500 traces and test set with 3 traces.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset[:3]\n",
    "train_labels = train_labels[:3]\n",
    "\n",
    "test_dataset = test_dataset[:3]\n",
    "test_labels = test_labels[:3]\n",
    "\n",
    "\n",
    "print(f'Created train set with {train_dataset.shape[0]} traces, validation set with {val_dataset.shape[0]} traces and test set with {test_dataset.shape[0]} traces.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standardize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = standardize(train_dataset).float(), standardize(val_dataset).float(), standardize(test_dataset).float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define The GP Regression Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2336.])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preds_to_onst(preds: torch.tensor):\n",
    "    # softargmax\n",
    "    beta = 100.0\n",
    "    softmax = torch.nn.functional.softmax(beta  * preds[:, 0, :], dim=-1)\n",
    "    indices = torch.arange(preds[:, 0, :].shape[-1])\n",
    "    return torch.sum(torch.mul(indices, softmax), dim=-1)\n",
    "\n",
    "\n",
    "x = torch.randn((1,3,3001))\n",
    "preds_to_onst(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define The GP Model Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train GP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eacad6dc05d64d828dfd6a215b03325f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "CPU times: user 10.7 s, sys: 56.9 ms, total: 10.8 s\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module =  gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "            # self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            #     gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()),\n",
    "            #     num_dims=3, grid_size=100\n",
    "            # )\n",
    "            self.picker = retraining_model\n",
    "            self.preds_to_onset = preds_to_onst\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            print('first line forward', x.shape)\n",
    "            print('x[:,:,0]', x[:,:,0])\n",
    "            projected_x = self.preds_to_onset(self.picker(x))\n",
    "            print(projected_x.shape)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "            print(projected_x.shape)\n",
    "            mean_x = self.mean_module(projected_x.clone())\n",
    "            print(mean_x.shape)\n",
    "            covar_x = self.covar_module(projected_x.clone())\n",
    "            print('covar_x', covar_x.shape)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_dataset, train_labels, likelihood)\n",
    "torch.allclose(model.train_inputs[0], train_dataset)\n",
    "training_iterations =  60\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.picker.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "def train():\n",
    "    iterator = tqdm.notebook.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        # print(train_dataset.shape)\n",
    "        output = model(train_dataset)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_labels).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "%time train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate GP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 3, 3001]), torch.Size([3, 3, 3001]))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape, train_dataset.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "first line forward torch.Size([3, 3, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202],\n",
      "        [ 1.0302,  0.5874,  0.7209],\n",
      "        [ 0.8320,  2.1147,  0.0345]])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "covar_x torch.Size([3, 3])\n",
      "first line forward torch.Size([3, 6, 3001])\n",
      "x[:,:,0] tensor([[ 0.5377,  0.3632, -0.7202,  1.5148,  0.6522,  0.3628],\n",
      "        [ 1.0302,  0.5874,  0.7209, -0.0891,  0.4517,  0.1493],\n",
      "        [ 0.8320,  2.1147,  0.0345,  0.8540, -0.4428, -0.0383]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [8, 3, 7], expected input[3, 6, 3001] to have 3 channels, but got 6 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad(), gpytorch\u001B[38;5;241m.\u001B[39msettings\u001B[38;5;241m.\u001B[39muse_toeplitz(\u001B[38;5;28;01mFalse\u001B[39;00m), gpytorch\u001B[38;5;241m.\u001B[39msettings\u001B[38;5;241m.\u001B[39mfast_pred_var():\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicting\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreds\u001B[39m\u001B[38;5;124m'\u001B[39m,preds)\n\u001B[1;32m      9\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m preds_to_onst(preds)\n",
      "File \u001B[0;32m~/miniconda3/envs/seis/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:306\u001B[0m, in \u001B[0;36mExactGP.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m     full_inputs\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mcat([train_input, \u001B[38;5;28minput\u001B[39m], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m    305\u001B[0m \u001B[38;5;66;03m# Get the joint distribution for training/test data\u001B[39;00m\n\u001B[0;32m--> 306\u001B[0m full_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mExactGP\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfull_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m settings\u001B[38;5;241m.\u001B[39mdebug()\u001B[38;5;241m.\u001B[39mon():\n\u001B[1;32m    308\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(full_output, MultivariateNormal):\n",
      "File \u001B[0;32m~/miniconda3/envs/seis/lib/python3.10/site-packages/gpytorch/module.py:30\u001B[0m, in \u001B[0;36mModule.__call__\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 30\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "Cell \u001B[0;32mIn[45], line 20\u001B[0m, in \u001B[0;36mGPRegressionModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst line forward\u001B[39m\u001B[38;5;124m'\u001B[39m, x\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx[:,:,0]\u001B[39m\u001B[38;5;124m'\u001B[39m, x[:,:,\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 20\u001B[0m projected_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreds_to_onset(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpicker\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(projected_x\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     22\u001B[0m projected_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_to_bounds(projected_x)  \u001B[38;5;66;03m# Make the NN values \"nice\"\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/seis/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/GIT/seisbench_phasenet_fix/seisbench/models/phasenet.py:105\u001B[0m, in \u001B[0;36mPhaseNet.forward\u001B[0;34m(self, x, logits)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 105\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_bn(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m    107\u001B[0m     skips \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (conv_same, bn1, conv_down, bn2) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_branch):\n",
      "File \u001B[0;32m~/miniconda3/envs/seis/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/seis/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/seis/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    307\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    308\u001B[0m                     _single(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 309\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Given groups=1, weight of size [8, 3, 7], expected input[3, 6, 3001] to have 3 channels, but got 6 channels instead"
     ]
    }
   ],
   "source": [
    "    model.eval()\n",
    "likelihood.eval()\n",
    "model.picker.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    print('predicting')\n",
    "    preds = model(test_dataset)\n",
    "    print('preds',preds)\n",
    "    prediction = preds_to_onst(preds)\n",
    "    print('prediction', prediction)\n",
    "    observed_pred = likelihood(prediction)\n",
    "    print(observed_pred.confidence_region())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lower, upper = observed_pred.confidence_region()\n",
    "lower, upper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Custom Datasets/DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def label_normal_smooth(label):\n",
    "    num_samples = NUM_SAMPLES\n",
    "    sigma = 1000.0\n",
    "    v = torch.arange(num_samples).double()\n",
    "    return (1.0/(sigma*torch.sqrt(2.0* torch.tensor(torch.pi)))) * torch.exp(-0.5*torch.square((v-label)/sigma))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset: torch.tensor, labels: torch.tensor, transform=None, target_transform=None):\n",
    "        self._dataset = dataset\n",
    "        self._labels = labels\n",
    "        assert dataset.dim() == 3, f'Expected 3 dim dataset tensor (#traces,#channels,#samples). Got {dataset.dim()} dims. Shape {dataset.shape} '\n",
    "        assert labels.shape[0] == dataset.shape[0], f'Expected 1 label per trace. Got {labels.shape[0]} for {dataset.shape[0]} traces'\n",
    "        self._len = int(dataset.shape[0])\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trace = self._dataset[idx]\n",
    "        label = self._labels[idx]\n",
    "        if self.transform:\n",
    "            trace = self.transform(trace)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return trace, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainset = CustomDataset(dataset=train_dataset, labels=train_labels, target_transform=lambda l: (l,label_normal_smooth(l)))\n",
    "valset = CustomDataset(dataset=val_dataset, labels=val_labels, target_transform=lambda l: (l,label_normal_smooth(l)))\n",
    "testset = CustomDataset(dataset=test_dataset, labels=test_labels, target_transform=lambda l: (l,label_normal_smooth(l)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Created train set with {len(trainset)} traces, validation set with {len(valset)} traces and test set with {len(testset)} traces.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taken from Seisbench tutorial notebook \"03a_training_phasenet\"  -  not using it for now - commented out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_true, eps=1e-5):\n",
    "    # vector cross entropy loss\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "\n",
    "    h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
    "\n",
    "    h = h.mean()  # Mean over batch axis\n",
    "\n",
    "    return -h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, large_error_threshold=100):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0.0\n",
    "    large_errors_counter = 0\n",
    "    mean_residual = 0.0\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        trace, (label, label_smoothed) = batch\n",
    "\n",
    "        # print('trace shape', trace.shape, 'label shape', label.shape, 'label_smoothed shape', label_smoothed.shape)\n",
    "\n",
    "        batch_size = trace.shape[0]\n",
    "        # print('batch_size', batch_size)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        # Fwd pass - outputs the likelihood function\n",
    "        pred_probs = model(trace.double().to(model.device))\n",
    "\n",
    "        if SBM_CLASS == sbm.EQTransformer:\n",
    "            # EQTransformer returns a tuple (N,Z,E)\n",
    "            pred_probs = torch.stack((pred_probs[1],pred_probs[0],pred_probs[2]), dim=0).swapaxes(0,1)\n",
    "        # print('pred_probs shape', pred_probs.shape)\n",
    "\n",
    "\n",
    "        # softargmax\n",
    "        beta = 100.0\n",
    "        softmax = torch.nn.functional.softmax(beta  * pred_probs[:, 0, :], dim=-1)\n",
    "        indices = torch.arange(pred_probs[:, 0, :].shape[-1])\n",
    "        softargmax_preds = torch.sum(torch.mul(indices, softmax), dim=-1)\n",
    "\n",
    "        loss = torch.abs(softargmax_preds - label).mean()\n",
    "\n",
    "        # loss = loss_fn(pred_probs[:,0,:], label_smoothed.double().to(model.device))\n",
    "        # print('loss', loss)\n",
    "        # loss = loss_fn(F.log_softmax(pred_probs[:,0,:], dim=-1), F.log_softmax(label.to(model.device), dim=-1))\n",
    "\n",
    "        prediction = torch.argmax(pred_probs[:, 0, :], dim=-1)\n",
    "        # print('prediction', prediction.shape)\n",
    "        residual = torch.abs(prediction - label.to(model.device))\n",
    "        # print(residual)\n",
    "        mean_residual += float(residual.mean())\n",
    "        # print('mean_residual', mean_residual)\n",
    "        # print(residual > large_error_threshold)\n",
    "        # print(residual[residual > large_error_threshold])\n",
    "        # print((residual > large_error_threshold).sum())\n",
    "        # large_errors_counter += (1 if residual > large_error_threshold else 0)\n",
    "        large_errors_counter += int((residual > large_error_threshold).sum())\n",
    "        # print(large_errors_counter)\n",
    "\n",
    "        # if batch_id == 2:\n",
    "        #     break\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # if batch_id % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch_id * trace.shape[0]\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    mean_residual /= num_batches\n",
    "    train_loss /= num_batches\n",
    "\n",
    "    # print(mean_residual)\n",
    "    # print(train_loss)\n",
    "    # raise Exception\n",
    "    return train_loss, mean_residual, large_errors_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(dataloader, model, loss_fn, large_error_threshold=100):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    large_errors_counter = 0\n",
    "    mean_residual = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            trace, (label, label_smoothed) = batch\n",
    "            pred_probs = model(trace.double().to(model.device))\n",
    "            if SBM_CLASS == sbm.EQTransformer:\n",
    "                # EQTransformer returns a tuple (N,Z,E)\n",
    "                pred_probs = torch.stack((pred_probs[1],pred_probs[0],pred_probs[2]), dim=0).swapaxes(0,1)\n",
    "            # Take the maximum of the z channel prediction\n",
    "            loss = loss_fn(pred_probs[:,0,:], label_smoothed.to(model.device))\n",
    "            # loss = loss_fn(F.log_softmax(pred_probs[:,0,:], dim=-1), F.log_softmax(label_smoothed.to(model.device), dim=-1))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            prediction = torch.argmax(pred_probs[:, 0, :], dim=-1)\n",
    "            residual = float(torch.abs(prediction - label.to(model.device)))\n",
    "            mean_residual += residual\n",
    "            large_errors_counter += (1 if residual > large_error_threshold else 0)\n",
    "\n",
    "    mean_residual /= num_batches\n",
    "    test_loss /= num_batches\n",
    "    return test_loss, mean_residual, large_errors_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(trainset, valset, benchmark_model, epochs, learning_rate, batch_size):\n",
    "    # init weights&biases monitoring\n",
    "    wandb.init(project=\"seisynth\", entity=\"moshebeutel\")\n",
    "    wandb.config = {\"learning_rate\": learning_rate, \"epochs\": epochs, \"batch_size\": batch_size}\n",
    "\n",
    "    trained_model = load_pretrained_model(model_class=SBM_CLASS, dataset_trained_on=dataset_origin).double()\n",
    "    train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_dataloader = DataLoader(valset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Define the train optimizer and optimization criterion\n",
    "    # optimizer = torch.optim.SGD(trained_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-3)\n",
    "    optimizer = torch.optim.Adam(trained_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # criterion = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "    # criterion = torch.nn.MSELoss()\n",
    "\n",
    "    criterion = loss_fn\n",
    "\n",
    "    # Evaluate the benchmark model on the validation data.\n",
    "    # The benchmark model is not training so it is done once.\n",
    "    if benchmark_model is not None:\n",
    "        benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter =  test_loop(dataloader=val_dataloader, model=benchmark_model, loss_fn=criterion, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "        print(f'Benchmark results: loss {benchmark_loss}, Mean Residual {benchmark_mean_residual}, Errors Above {LARGE_ERROR_THRESHOLD_SECONDS} sec. {benchmark_large_errors_counter}')\n",
    "\n",
    "    best_val_mean_residual = 10000\n",
    "\n",
    "    pbar = tqdm(range(epochs))\n",
    "    for t in pbar:\n",
    "        epoch_train_loss, epoch_train_mean_residual, epoch_train_large_errors_counter = train_loop(dataloader=train_dataloader, model=trained_model, loss_fn=criterion , optimizer=optimizer, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "        epoch_val_loss, epoch_val_mean_residual, epoch_val_large_errors_counter = test_loop(dataloader=val_dataloader, model=trained_model, loss_fn=criterion, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "\n",
    "        wandb.log({'epoch train loss': epoch_train_loss,\n",
    "                   'epoch_train_mean_residual':epoch_train_mean_residual,\n",
    "                   'epoch_train_large_errors_counter':epoch_train_large_errors_counter,\n",
    "                   'epoch validation loss': epoch_val_loss,\n",
    "                   'epoch_val_mean_residual':epoch_val_mean_residual,\n",
    "                   'epoch_val_large_errors_counter':epoch_val_large_errors_counter})\n",
    "\n",
    "        if epoch_val_mean_residual < best_val_mean_residual:\n",
    "            best_val_mean_residual = epoch_val_mean_residual\n",
    "            trained_model.save(path=f'./mean_residual_{epoch_val_mean_residual}_lr_{learning_rate}_batch_{batch_size}')\n",
    "\n",
    "\n",
    "        pbar.set_description(f'Epoch {t}, train loss {epoch_train_loss}, validation loss {epoch_val_loss}, epoch_val_large_errors_counter {epoch_val_large_errors_counter}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Sweep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#     'method': 'grid'\n",
    "#     }\n",
    "# parameters_dict = {}\n",
    "#\n",
    "# sweep_config['parameters'] = parameters_dict\n",
    "# metric = {\n",
    "#     'name': 'epoch_val_mean_residual',\n",
    "#     'goal': 'minimize'\n",
    "#     }\n",
    "#\n",
    "# sweep_config['metric'] = metric\n",
    "#\n",
    "# # parameters_dict.update({\n",
    "# #     'epochs': {\n",
    "# #         'value': 50},\n",
    "# #     })\n",
    "#\n",
    "# parameters_dict.update({\n",
    "#       'learning_rate_log': {\n",
    "#           'values': [-7, -6, -5, -4]\n",
    "#       },\n",
    "#       'batch_size': {\n",
    "#           'values': [16, 32, 64, 128]\n",
    "#       }\n",
    "#     })\n",
    "#\n",
    "# def sweep_train(config=None):\n",
    "#\n",
    "#     with wandb.init(config=config):\n",
    "#         config = wandb.config\n",
    "#\n",
    "#         learning_rate = 10 ** config.learning_rate_log\n",
    "#\n",
    "#         batch_size=config.batch_size\n",
    "#         epochs = 13\n",
    "#\n",
    "#         train(trainset=trainset, valset=valset, benchmark_model=None, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size)\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"seisynth\")\n",
    "# wandb.agent(sweep_id, sweep_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for (batch_size,lr) in [(64,1e-4), (64, 1e-3)]:\n",
    "#     train(trainset=trainset, valset=valset, benchmark_model=pretrained_model.double(), epochs=15, learning_rate=lr, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Call Train Entry Point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train(trainset=trainset, valset=valset, benchmark_model=pretrained_model.double(), epochs=EPOCHS, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# # Large Error Traces\n",
    "# original_dataset_path = os.path.join(DATASET_PATH, 'le_original_dataset.pt')\n",
    "# assert_path_exists(path_str=original_dataset_path)\n",
    "# original_labels_path = os.path.join(DATASET_PATH, 'le_original_labels.pt')\n",
    "# assert_path_exists(path_str=original_labels_path)\n",
    "# bounds = [-20, 0, 2, 5, 10, 20, 100]\n",
    "# j=5\n",
    "# suffix = f'_{dataset_origin}_bounds_{bounds[j]}_{bounds[j+1]}.pt'\n",
    "#\n",
    "# original_dataset_path = os.path.join(DATASET_PATH, 'traces' + suffix)\n",
    "# assert_path_exists(path_str=original_dataset_path)\n",
    "# original_labels_path = os.path.join(DATASET_PATH, 'labels' + suffix)\n",
    "# assert_path_exists(path_str=original_labels_path)\n",
    "#\n",
    "# original_dataset, original_labels = load_dataset_and_labels(dataset_path=original_dataset_path, labels_path=original_labels_path)\n",
    "\n",
    "# original_dataset, original_labels = original_snr_binned_dataset_left_for_test, original_snr_binned_labels_left_for_test\n",
    "# original_dataset, original_labels = test_dataset, test_labels\n",
    "#\n",
    "# valset_original = CustomDataset(dataset=original_dataset.float(), labels=original_labels.float(), target_transform=lambda l: (l,label_normal_smooth(l)))\n",
    "#\n",
    "# benchmark_dataloader = DataLoader(valset_original, batch_size=1, shuffle=False)\n",
    "# print(len(valset_original))\n",
    "# criterion = loss_fn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter =  test_loop(dataloader=benchmark_dataloader, model=pretrained_model, loss_fn=criterion, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "#\n",
    "# benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter, float(benchmark_large_errors_counter) / float(len(benchmark_dataloader))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# retraining_model =  SBM_CLASS.load('/home/moshe/GIT/summer_2022_Seismology/models/GFZ/retrains_on_mixed_real_and_synthetic_datasets/phasenet_geofon/mean_residual_54.69584352631581_lr_0.0001_batch_16')\n",
    "#\n",
    "\n",
    "# trained on mixed  30000 - snr binned and noisy\n",
    "# retraining_model =  SBM_CLASS.load('/home/moshe/GIT/summer_2022_Seismology/models/GFZ/retrains_on_mixed_real_train_and_synthetic_datasets/mean_residual_57.52923886111117_lr_0.0001_batch_16')\n",
    "\n",
    "# trained on real  15000  snr binned only\n",
    "retraining_model =  SBM_CLASS.load('/home/moshe/GIT/summer_2022_Seismology/notebooks/mean_residual_38.62195977777774_lr_0.0001_batch_16')\n",
    "\n",
    "\n",
    "# trained on  4250 noisy only\n",
    "# retraining_model =  SBM_CLASS.load('/home/moshe/GIT/summer_2022_Seismology/notebooks/mean_residual_44.00933950000001_lr_0.0001_batch_16')\n",
    "\n",
    "\n",
    "# trained on  15000 noisy only  standardize\n",
    "retraining_model =  SBM_CLASS.load('/home/moshe/GIT/summer_2022_Seismology/notebooks/mean_residual_55.95261939999995_lr_0.0001_batch_16')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter =  test_loop(dataloader=benchmark_dataloader, model=retraining_model, loss_fn=criterion, large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "#\n",
    "# benchmark_loss, benchmark_mean_residual, benchmark_large_errors_counter, float(benchmark_large_errors_counter) / float(len(benchmark_dataloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seisbench.data as sbd\n",
    "import pandas as pd\n",
    "import seisbench.generate as sbg\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "# 'stead', 'geofon', 'ethz', 'iquique'\n",
    "dataset_origin = 'ethz'\n",
    "SBD_CLASSES={'ethz':sbd.ETHZ, 'geofon':sbd.GEOFON, 'iquique': sbd.Iquique, 'stead':sbd.STEAD}\n",
    "SBD_CLASS=SBD_CLASSES[dataset_origin]\n",
    "NUM_SAMPLES=3001\n",
    "PHASE_LABEL = 'P' # 'S'\n",
    "SAMPLING_RATE = 100\n",
    "SNR_THRESHOLD = 20\n",
    "BATCH_SIZE=1000\n",
    "TARGETS_PATH = f'/home/moshe/datasets/GFZ/targets/{dataset_origin}/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def residual_to_cell(residual: float)->int:\n",
    "    return  round((residual/100) + 0.00001) + 10\n",
    "def cell_to_residual(cell: int)->int:\n",
    "    return cell - 10\n",
    "\n",
    "cell_to_residual(residual_to_cell(-150))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    residuals_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            trace, (label, label_smoothed) = batch\n",
    "            trace = standardize(trace)\n",
    "            pred_probs = model(trace.to(model.device))\n",
    "            if SBM_CLASS == sbm.EQTransformer:\n",
    "                # EQTransformer returns a tuple (N,Z,E)\n",
    "                pred_probs = torch.stack((pred_probs[1],pred_probs[0],pred_probs[2]), dim=0).swapaxes(0,1)\n",
    "            # Take the maximum of the z channel prediction\n",
    "            loss = loss_fn(pred_probs[:,0,:], label_smoothed.to(model.device))\n",
    "            # loss = loss_fn(F.log_softmax(pred_probs[:,0,:], dim=-1), F.log_softmax(label_smoothed.to(model.device), dim=-1))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            prediction = torch.argmax(pred_probs[:, 0, :], dim=-1)\n",
    "            residual = float(prediction - label.to(model.device))\n",
    "            residuals_list.append(residual)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    return test_loss, residuals_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !mkdir targets\n",
    "# !mkdir targets/ethz\n",
    "# !mkdir targets/geofon\n",
    "# !mkdir targets/iquique\n",
    "# !mkdir targets/stead\n",
    "\n",
    "# #ethz\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/ethz/task1.csv\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/ethz/task23.csv\n",
    "# !mv *.csv targets/ethz\n",
    "#\n",
    "# #geofon\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/geofon/task1.csv\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/geofon/task23.csv\n",
    "# !mv *.csv targets/geofon\n",
    "\n",
    "#iquique\n",
    "# NO TARGETS!!!!\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/iquique/task1.csv\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/iquique/task23.csv\n",
    "# !mv *.csv targets/iquique\n",
    "\n",
    "#stead\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/stead/task1.csv\n",
    "# !wget https://dcache-demo.desy.de:2443/Helmholtz/HelmholtzAI/SeisBench/auxiliary/pick-benchmark/targets/stead/task23.csv\n",
    "# !mv *.csv targets/stead"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = SBD_CLASS(sampling_rate=SAMPLING_RATE, force=True).train()\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "targets_task23 = pd.read_csv(os.path.join(TARGETS_PATH,'task23.csv'))\n",
    "merged_metadata = pd.merge(data.metadata, targets_task23, on='trace_name')\n",
    "requested_event_list=[]\n",
    "filtered_metadata = merged_metadata[(merged_metadata.phase_label == PHASE_LABEL) ]\n",
    "if requested_event_list:\n",
    "  filtered_metadata = filtered_metadata[filtered_metadata.source_id.isin(requested_event_list)]\n",
    "else:\n",
    "  print('All events will contribute to the resulting dataset')\n",
    "\n",
    "gen  = sbg.SteeredGenerator(data, filtered_metadata )\n",
    "print(len(gen))\n",
    "augmentations = [\n",
    "            sbg.ChangeDtype(np.float32),\n",
    "            sbg.Normalize(demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "            sbg.SteeredWindow(windowlen=NUM_SAMPLES, strategy=\"pad\")\n",
    "        ]\n",
    "\n",
    "gen.add_augmentations(augmentations)\n",
    "\n",
    "@gen.augmentation\n",
    "def get_arrival_sample(state_dict):\n",
    "  _, metadata = state_dict[\"X\"]\n",
    "  key = f\"trace_{state_dict['_control_']['full_phase_label']}_arrival_sample\"\n",
    "  state_dict['station_code'] = (metadata['station_code'], key)\n",
    "  state_dict[\"onset_sample\"] = (metadata[key], None)\n",
    "\n",
    "num_dataset_traces = int(len(gen))\n",
    "\n",
    "print(f'There are {num_dataset_traces} traces in the resulting dataset.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "loader =  DataLoader(gen, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "print(len(loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum_traces, sum_benchmark_loss, sum_benchmark_mean_residual,  sum_retrained_loss, sum_retrained_mean_residual = 0,0,0,0,0\n",
    "num_thresholds = 20\n",
    "le_thresholds = list(range(100,100*num_thresholds + 1,100))\n",
    "benchmark_residuals_list = []\n",
    "retrained_residuals_list = []\n",
    "sum_retrained_large_errors_counters = {th:0 for th in le_thresholds}\n",
    "for batch_idx, dataset_dict in enumerate(loader):\n",
    "\n",
    "    traces=dataset_dict['X']\n",
    "    labels = dataset_dict['onset_sample']\n",
    "\n",
    "    criterion = loss_fn\n",
    "\n",
    "    valset_original = CustomDataset(dataset=traces.float(), labels=labels.float(), target_transform=lambda l: (l,label_normal_smooth(l)))\n",
    "\n",
    "    benchmark_dataloader = DataLoader(valset_original, batch_size=1, shuffle=False)\n",
    "\n",
    "    benchmark_loss, batch_benchmark_residuals_list =  evaluate_loop(dataloader=benchmark_dataloader, model=pretrained_model, loss_fn=criterion)\n",
    "                                                                                              # large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "    retrained_loss, batch_retrained_residuals_list =  evaluate_loop(dataloader=benchmark_dataloader, model=retraining_model, loss_fn=criterion)\n",
    "                                                                                              # large_error_threshold=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "\n",
    "    benchmark_residuals_list.extend(batch_benchmark_residuals_list)\n",
    "    retrained_residuals_list.extend(batch_retrained_residuals_list)\n",
    "    benchmark_residual_tensor =   torch.tensor(benchmark_residuals_list)\n",
    "    benchmark_residuals_rmse = torch.sqrt(torch.mean(torch.square(benchmark_residual_tensor)))\n",
    "    benchmark_residual_median = torch.median(benchmark_residual_tensor)\n",
    "    retrained_residual_tensor =  torch.tensor(retrained_residuals_list)\n",
    "    retrained_residuals_rmse = torch.sqrt(torch.mean(torch.square(retrained_residual_tensor)))\n",
    "    retrained_residuals_median = torch.median(retrained_residual_tensor)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'****   Batch No. {batch_idx}   {traces.shape[0]} traces ****')\n",
    "    print(f'Benchmark root mean squared residual  {benchmark_residuals_rmse}, large error counters {torch.histogram(benchmark_residual_tensor, bins=5)}')\n",
    "    print(f'Retrained root mean squared residual {retrained_residuals_rmse}, large error counters {torch.histogram(retrained_residual_tensor, bins=5)}')\n",
    "    sum_traces+=traces.shape[0]\n",
    "\n",
    "print('EVALUATION FINISHED', sum_traces, 'traces')\n",
    "\n",
    "benchmark_residual_tensor =   torch.tensor(benchmark_residuals_list)\n",
    "benchmark_residuals_rmse = torch.sqrt(torch.mean(torch.square(benchmark_residual_tensor)))\n",
    "benchmark_residual_median = torch.median(benchmark_residual_tensor)\n",
    "retrained_residual_tensor =  torch.tensor(retrained_residuals_list)\n",
    "retrained_residuals_rmse = torch.sqrt(torch.mean(torch.square(retrained_residual_tensor)))\n",
    "retrained_residuals_median = torch.median(retrained_residual_tensor)\n",
    "print(f'Benchmark root mean squared residual  {benchmark_residuals_rmse}, median {benchmark_residual_median} ')\n",
    "print(f'Retrained root mean squared residual {retrained_residuals_rmse}, median {retrained_residuals_median} ')\n",
    "\n",
    "benchmark_hist, benchmark_bins = torch.histogram(benchmark_residual_tensor, bins=30)\n",
    "retrained_hist, retrained_bins = torch.histogram(retrained_residual_tensor, bins=30)\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,8))\n",
    "ax1.set_title('Benchmark Model Residuals')\n",
    "ax2.set_title('Retrained Model Residuals')\n",
    "ax1.stairs(benchmark_hist, benchmark_bins)\n",
    "ax2.stairs(retrained_hist, retrained_bins)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "benchmark_hist, benchmark_bins = torch.histogram(benchmark_residual_tensor, bins=10)\n",
    "retrained_hist, retrained_bins = torch.histogram(retrained_residual_tensor, bins=10)\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,8), sharey='all', sharex='all')\n",
    "ax1.set_title('Benchmark Model Residuals')\n",
    "ax2.set_title('Retrained Model Residuals')\n",
    "ax1.stairs(benchmark_hist, benchmark_bins)\n",
    "ax2.stairs(retrained_hist, retrained_bins)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(sum_benchmark_large_errors_counters.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(sum_retrained_large_errors_counters.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1,2, sharey='all', figsize=(12,8))\n",
    "# ax1.stairs(list(sum_benchmark_large_errors_counters.values()),list(sum_benchmark_large_errors_counters.keys())+[1100]);\n",
    "# ax1.set_title('Pretrained - Benchmark')\n",
    "# ax2.stairs(list(sum_retrained_large_errors_counters.values()),list(sum_retrained_large_errors_counters.keys())+[1100]);\n",
    "# ax2.set_title('Retrained')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sum_benchmark_loss/sum_traces,  sum_retrained_loss/sum_traces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sum_benchmark_mean_residual/len(loader), sum_retrained_mean_residual/len(loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# len(loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sum_benchmark_large_errors_counter*100/sum_traces,  sum_retrained_large_errors_counter*100/sum_traces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stead_data = sbd.STEAD(sampling_rate=SAMPLING_RATE, force=True).test()\n",
    "# print(stead_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# le_original_dataset_path = os.path.join(DATASET_PATH, 'le_original_dataset.pt')\n",
    "# assert_path_exists(path_str=le_original_dataset_path)\n",
    "# le_original_labels_path = os.path.join(DATASET_PATH, 'le_original_labels.pt')\n",
    "# assert_path_exists(path_str=le_original_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# le_original_dataset, le_original_labels = load_dataset_and_labels(dataset_path=le_original_dataset_path, labels_path=le_original_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(f'Loaded {le_original_dataset.shape[0]} traces')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
