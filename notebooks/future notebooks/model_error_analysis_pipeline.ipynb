{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model Error Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The purpose of this notebook is to examine the traces for which the model performed worse. In particular, consider the 10dB dataset generated using ETHZ and Joachim Noises and look at the traces where the pretrained model performed worse than a second away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparations - Imports, Config, Data and model loading etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seisbench.models as sbm\n",
    "import torch\n",
    "from evaluation.noisy_dataset_evaluation import find_large_error_traces\n",
    "from experiments.filtering import fft_filter_experiment\n",
    "from utils.common import load_dataset_and_labels, load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Possible values\n",
    "DATASETS_ORIGINS = ['ethz', 'geofon']\n",
    "SBM_CLASSES= [sbm.PhaseNet, sbm.EQTransformer]\n",
    "MODEL_TO_NUM_SAMPLES = {sbm.EQTransformer:6000, sbm.PhaseNet: 3001}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset_origin = 'ethz'\n",
    "assert dataset_origin in DATASETS_ORIGINS, f'Expected dataset one of {DATASETS_ORIGINS}. Got {dataset_origin}.'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "seisbench.models.eqtransformer.EQTransformer"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SBM_CLASS= sbm.EQTransformer\n",
    "assert SBM_CLASS in SBM_CLASSES\n",
    "SBM_CLASS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "6000"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES=MODEL_TO_NUM_SAMPLES[SBM_CLASS]\n",
    "NUM_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "NUM_SHIFTS=5\n",
    "SAMPLE_RATE=100\n",
    "LARGE_ERROR_THRESHOLD_SECONDS=1\n",
    "LARGE_ERROR_THRESHOLD_SAMPLES=LARGE_ERROR_THRESHOLD_SECONDS*SAMPLE_RATE\n",
    "SYNTHESIZED_SNR=10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "('/home/moshe/datasets/GFZ/noisy_datasets/ethz_6000_sample_joachim_noises_energy_ratio_snr/',\n '/home/moshe/datasets/GFZ/noisy_datasets/ethz_6000_sample_joachim_noises_energy_ratio_snr/noisy_dataset_snr_10')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHIFTING_PLOTS_PATH='/home/moshe/plots/shifting_plots'\n",
    "DATASET_PATH=f'/home/moshe/datasets/GFZ/noisy_datasets/{dataset_origin}_{NUM_SAMPLES}_sample_joachim_noises_energy_ratio_snr/'\n",
    "NOISY_DATA_PATH = os.path.join(DATASET_PATH, f'noisy_dataset_snr_{SYNTHESIZED_SNR}')\n",
    "DATASET_PATH, NOISY_DATA_PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def assert_path_exists(path_str: str, name: str=''):\n",
    "    assert os.path.exists(path_str), f'{name} {path_str} does not exist'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "assert_path_exists(path_str=DATASET_PATH, name='DATASET_PATH')\n",
    "assert_path_exists(path_str=NOISY_DATA_PATH, name='NOISY_DATA_PATH')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the original High SNR Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a dataset of high SNR traces taken from the original ETHZ\\GEOFON dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1856 traces and corresponding labels.\n"
     ]
    }
   ],
   "source": [
    "# SAVE_TO_PATH=f'/home/moshe/datasets/GFZ/noisy_datasets/all_traces_{dataset_origin}_{NUM_SAMPLES}_sample_joachim_noises_energy_ratio_snr/'\n",
    "\n",
    "dataset_traces_path = os.path.join(DATASET_PATH, 'original_dataset.pt')\n",
    "dataset_labels_path = os.path.join(DATASET_PATH, 'original_labels.pt')\n",
    "original_dataset = torch.load(dataset_traces_path)   #[:num_traces]\n",
    "original_labels = torch.load(dataset_labels_path)    #[:num_traces]\n",
    "\n",
    "num_original_traces = original_dataset.shape[0]\n",
    "num_original_labels = original_labels.shape[0]\n",
    "num_original_samples = original_dataset.shape[-1]\n",
    "\n",
    "assert num_original_labels == num_original_traces, f'Expected traces equal num labels.Got {num_original_traces} traces and {num_original_labels} labels'\n",
    "assert num_original_samples == NUM_SAMPLES, f'Expected {NUM_SAMPLES} in each trace. Got {num_original_samples}.'\n",
    "\n",
    "print(f'Loaded {num_original_traces} traces and corresponding labels.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Synthetic Noisy Traces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "synthetic_noisy_dataset_path= os.path.join(NOISY_DATA_PATH, 'traces.pt')\n",
    "\n",
    "synthetic_noisy_labels_path= os.path.join(NOISY_DATA_PATH, 'labels.pt')\n",
    "\n",
    "full_noises_path = os.path.join(NOISY_DATA_PATH, 'full_noise_traces.pt')\n",
    "\n",
    "factors_path = os.path.join(NOISY_DATA_PATH, 'factors.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "assert_path_exists(path_str=synthetic_noisy_dataset_path, name='synthetic_noisy_dataset_path')\n",
    "assert_path_exists(path_str=synthetic_noisy_labels_path, name='synthetic_noisy_labels_path')\n",
    "assert_path_exists(path_str=full_noises_path, name='full_noises_path')\n",
    "assert_path_exists(path_str=factors_path, name='factors_path')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded dataset has 14848 traces, each has 3 channels of 6000 samples. Each entry is of type torch.float32\n",
      "The loaded labels have 14848 labels. Each entry is of type torch.float32\n"
     ]
    }
   ],
   "source": [
    "synthetic_noisy_dataset, synthetic_noisy_labels = load_dataset_and_labels(dataset_path=synthetic_noisy_dataset_path, labels_path=synthetic_noisy_labels_path)\n",
    "synthetic_noisy_dataset, synthetic_noisy_labels = synthetic_noisy_dataset.float(), synthetic_noisy_labels.float()\n",
    "assert NUM_SAMPLES == synthetic_noisy_dataset.shape[-1], f'Expected Dataset contain {NUM_SAMPLES} samples. Got {synthetic_noisy_dataset.shape[-1]}'\n",
    "\n",
    "print(f'The loaded dataset has {synthetic_noisy_dataset.shape[0]} traces, each has {synthetic_noisy_dataset.shape[1]} channels of {synthetic_noisy_dataset.shape[2]} samples. Each entry is of type {synthetic_noisy_dataset.dtype}')\n",
    "\n",
    "print(f'The loaded labels have {synthetic_noisy_labels.shape[0]} labels. Each entry is of type {synthetic_noisy_labels.dtype}')\n",
    "\n",
    "num_original_traces = synthetic_noisy_dataset.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load a Pretrained Phasenet Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with <class 'seisbench.models.eqtransformer.EQTransformer'> on ETHZ\n",
      "Load <class 'seisbench.models.eqtransformer.EQTransformer'> pretrained weights\n",
      "<class 'seisbench.models.eqtransformer.EQTransformer'> pretrained keys ['ethz', 'geofon', 'instance', 'iquique', 'lendb', 'neic', 'obs', 'original', 'original_nonconservative', 'scedc', 'stead']\n"
     ]
    },
    {
     "data": {
      "text/plain": "EQTransformer(\n  (encoder): Encoder(\n    (convs): ModuleList(\n      (0): Conv1d(3, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      (1): Conv1d(8, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (3): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (5): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n    )\n    (pools): ModuleList(\n      (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (res_cnn_stack): ResCNNStack(\n    (members): ModuleList(\n      (0): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (1): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (2): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (3): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (4): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n      (5): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (6): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n    )\n  )\n  (bi_lstm_stack): BiLSTMStack(\n    (members): ModuleList(\n      (0): BiLSTMBlock(\n        (lstm): LSTM(64, 16, bidirectional=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BiLSTMBlock(\n        (lstm): LSTM(16, 16, bidirectional=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BiLSTMBlock(\n        (lstm): LSTM(16, 16, bidirectional=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (transformer_d0): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (transformer_d): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (decoder_d): Decoder(\n    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n    (convs): ModuleList(\n      (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n    )\n  )\n  (conv_d): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  (dropout): Dropout(p=0.1, inplace=False)\n  (pick_lstms): ModuleList(\n    (0): LSTM(16, 16)\n    (1): LSTM(16, 16)\n  )\n  (pick_attentions): ModuleList(\n    (0): SeqSelfAttention()\n    (1): SeqSelfAttention()\n  )\n  (pick_decoders): ModuleList(\n    (0): Decoder(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): ModuleList(\n        (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n        (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n        (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n        (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n        (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n        (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      )\n    )\n    (1): Decoder(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): ModuleList(\n        (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n        (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n        (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n        (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n        (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n        (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      )\n    )\n  )\n  (pick_convs): ModuleList(\n    (0): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n    (1): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  )\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = load_pretrained_model(model_class=SBM_CLASS, dataset_trained_on=dataset_origin)\n",
    "pretrained_model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# NOISES_DATASET_PT= 'noises_dataset.pt'\n",
    "# NOISES_PATH='/home/moshe/GIT/summer_2022_Seismology/notebooks/Noises'\n",
    "# full_noise_traces = get_n_random_noises(num_noises=num_traces, desired_window_size=NUM_SAMPLES + NUM_SHIFTS * SAMPLE_RATE, noises_path=NOISES_PATH, filename=NOISES_DATASET_PT)\n",
    "#\n",
    "#\n",
    "# # Take the leftmost NUM_SAMPLES samples. The rest is left for shifts\n",
    "# noises=full_noise_traces[:,:NUM_SAMPLES]\n",
    "#\n",
    "# full_noise_traces.shape, noises.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the Loaded Datasets by The Pretrained Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# evaluation function for EqTransformer. Model returns a tuple instead of a tensor. For P phase should look at the tensor at index =1 of the tuple\n",
    "def eval_batch_tuple_return(batch, model):\n",
    "    with torch.no_grad():\n",
    "        pred = model(batch)\n",
    "        # transform the returned tuple to the same shape as phasenet where channel 0 is the p phase characteristic function\n",
    "        # pred = torch.stack(pred, dim=0)\n",
    "        if SBM_CLASS == sbm.EQTransformer:\n",
    "            # EQTransformer returns a tuple (N,Z,E)\n",
    "            pred = torch.stack((pred[1],pred[0],pred[2]), dim=0).swapaxes(0,1)\n",
    "        pred = pred.cpu()\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate both the original traces and the synthetic noisy traces and save the large error traces - the traces where the model had picking error (residual) larger than  predefined threshold (1 second by default - 100 samples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1856/1856 [00:38<00:00, 47.84it/s]\n",
      "100%|██████████| 14848/14848 [05:06<00:00, 48.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# large_error_traces_index_list = search_large_errors_given_desired_snr(model=pretrained_model, dataset=natural_dataset, labels=natural_dataset_labels, noise_traces=noises, desired_snr=SYNTHESIZED_SNR, calc_snr=CalcSNR(SnrCalcStrategy.ENERGY_RATIO))\n",
    "#\n",
    "large_error_traces_index_list_original_dataset = find_large_error_traces(dataset=original_dataset, model=pretrained_model.float(), labels=original_labels, threshold_samples=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "\n",
    "\n",
    "large_error_traces_index_list_synthetic_noisy_dataset = find_large_error_traces(dataset=synthetic_noisy_dataset, model=pretrained_model.float(), labels=synthetic_noisy_labels, threshold_samples=LARGE_ERROR_THRESHOLD_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 456 large errors in the noisy dataset\n",
      "There are 43 large errors in the original dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_len = int(len(large_error_traces_index_list_synthetic_noisy_dataset))\n",
    "\n",
    "print(f'There are {dataset_len} large errors in the noisy dataset')\n",
    "\n",
    "dataset_len_original = int(len(large_error_traces_index_list_original_dataset))\n",
    "\n",
    "print(f'There are {dataset_len_original} large errors in the original dataset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "le_original_dataset = original_dataset[large_error_traces_index_list_original_dataset]\n",
    "le_original_labels = original_labels[large_error_traces_index_list_original_dataset]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "le_dataset_traces_path = os.path.join(DATASET_PATH, 'le_original_dataset.pt')\n",
    "le_dataset_labels_path = os.path.join(DATASET_PATH, 'le_original_labels.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "torch.save(le_original_dataset, le_dataset_traces_path)\n",
    "torch.save(le_original_labels, le_dataset_labels_path)\n",
    "\n",
    "# le_original_dataset = torch.load(le_dataset_traces_path)\n",
    "# le_original_labels = torch.load(le_dataset_labels_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([43, 3, 6000]), torch.Size([43]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_original_dataset.shape, le_original_labels.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le_dataset = synthetic_noisy_dataset[large_error_traces_index_list_synthetic_noisy_dataset]\n",
    "le_labels = synthetic_noisy_labels[large_error_traces_index_list_synthetic_noisy_dataset]\n",
    "# le_noises_full = full_noise_traces[large_error_traces_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "le_dataset_path= os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_dataset.pt')\n",
    "\n",
    "le_labels_path= os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_labels.pt')\n",
    "\n",
    "le_full_noises_path = os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_full_noise_traces.pt')\n",
    "\n",
    "le_factors_path = os.path.join(NOISY_DATA_PATH, f'le_{str(SBM_CLASS)}_factors.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "full_noise_traces = torch.load(full_noises_path)\n",
    "factors = torch.load(factors_path)\n",
    "le_noises_full = full_noise_traces[large_error_traces_index_list_synthetic_noisy_dataset]\n",
    "le_factors = factors[large_error_traces_index_list_synthetic_noisy_dataset]\n",
    "torch.save(le_dataset, le_dataset_path)\n",
    "torch.save(le_labels, le_labels_path)\n",
    "torch.save(le_noises_full, le_full_noises_path)\n",
    "torch.save(le_factors, le_factors_path)\n",
    "\n",
    "\n",
    "# le_dataset = torch.load(le_dataset_path)\n",
    "# le_labels = torch.load(le_labels_path)\n",
    "# le_noises_full = torch.load(le_full_noises_path)\n",
    "# le_factors = torch.load(le_factors_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([456, 3, 6000]),\n torch.Size([456]),\n torch.Size([456, 3, 6600]),\n torch.Size([456]))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_dataset.shape, le_labels.shape, le_noises_full.shape, le_factors.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shifting scheme for large error cases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "num_traces_in_experiment=3\n",
    "# le_dataset[:num_traces_in_experiment].shape, le_labels[:num_traces_in_experiment].shape, le_noises_full[:num_traces_in_experiment].shape\n",
    "\n",
    "traces_list = [le_dataset[i] - le_factors[i]*le_noises_full[i,:,:NUM_SAMPLES] for i in range(num_traces_in_experiment)]\n",
    "labels_list = [le_labels[i] for i in range(num_traces_in_experiment)]\n",
    "full_noises_list = [le_noises_full[i] for i in range(num_traces_in_experiment)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for trace, full_noise_trace, label in zip(traces_list, full_noises_list, labels_list):\n",
    "#     shifting_experiment(trace=trace, full_noise_trace=full_noise_trace, label=int(label), \\\n",
    "#          model=pretrained_model.double(), num_shifts=NUM_SHIFTS, synthesized_snr=SYNTHESIZED_SNR, silent_prints=True,\n",
    "#                         save_plot_to='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filter the large error traces using FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# le_dataset = torch.load(le_dataset_path)\n",
    "# le_labels = torch.load(le_labels_path)\n",
    "\n",
    "# noised_traces_list = create_noisy_traces(calc_snr=CalcSNR(SnrCalcStrategy.ENERGY_RATIO), dataset=le_dataset, desired_snr=SYNTHESIZED_SNR, labels=le_labels, noise_traces=noises[large_error_traces_index_list])\n",
    "\n",
    "# dataset_len = le_original_dataset.shape[0]\n",
    "# noised_traces_list=[le_original_dataset[i,:,:].float() for i in range(dataset_len)]\n",
    "\n",
    "# dataset_len = le_dataset.shape[0]\n",
    "# noised_traces_list=[le_dataset[i,:,:].float() for i in range(dataset_len)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def band_filter_experiment(lco, uco):\n",
    "    print(f'lower_cut_off={lco}, upper_cut_off={uco}. Filtering {dataset_len} traces')\n",
    "    fixed = fft_filter_experiment(traces_list=noised_traces_list, label_list=le_labels, model=pretrained_model.float(),\n",
    "                                  sample_rate=SAMPLE_RATE, residual_threshold_seconds=1, lower_cut_off=lco,\n",
    "                                  upper_cut_off=uco, silent_prints=True, plot_fixed_traces=True)\n",
    "    print(fixed, 'fixed, out of', dataset_len )\n",
    "    return fixed\n",
    "\n",
    "def cut_off_search():\n",
    "    max_fixed = 0\n",
    "    best_lco_uco = []\n",
    "    for lco in range(20):\n",
    "        for uco in range(80, 101):\n",
    "            fixed = band_filter_experiment(lco, uco)\n",
    "            if fixed > max_fixed:\n",
    "                max_fixed = fixed\n",
    "                best_lco_uco = [(lco, uco)]\n",
    "            if fixed == max_fixed:\n",
    "                best_lco_uco.append((lco, uco))\n",
    "    print(f'cut off {best_lco_uco} fixed {max_fixed}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# cut_off_search()\n",
    "# band_filter_experiment(lco=12, uco=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa76d2c83586c95486e2cc3c656ad2d093b47aefbf53fc633acb9a860f5157ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
