{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seisbench.models as sbm\n",
    "import torch\n",
    "\n",
    "from dataset_creation.utils import remove_traces_not_to_use\n",
    "from evaluation.noisy_dataset_evaluation import find_large_error_traces, eval_mse, batch_max_values_and_residuals\n",
    "from utils.common import load_dataset_and_labels, load_pretrained_model, assert_path_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements and Configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is your user root directory?  (`/home/<username>/` on linux machines e.g.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USER_ROOT_DIR='/home/moshe/'\n",
    "assert_path_exists(path_str=USER_ROOT_DIR, name='USER_ROOT_DIR')\n",
    "USER_ROOT_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is the root folder of your datasets?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASETS_ROOT_DIR= os.path.join(USER_ROOT_DIR,'datasets/GFZ/')\n",
    "assert_path_exists(path_str=DATASETS_ROOT_DIR, name='DATASETS_ROOT_DIR')\n",
    "DATASETS_ROOT_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Possible values\n",
    "DATASETS_ORIGINS = ['ethz', 'geofon']\n",
    "SBM_CLASSES= [sbm.PhaseNet, sbm.EQTransformer]\n",
    "MODEL_TO_NUM_SAMPLES = {sbm.EQTransformer:6000, sbm.PhaseNet: 3001}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_origin = 'ethz'\n",
    "assert dataset_origin in DATASETS_ORIGINS, f'Expected dataset one of {DATASETS_ORIGINS}. Got {dataset_origin}.'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SBM_CLASS= sbm.PhaseNet\n",
    "assert SBM_CLASS in SBM_CLASSES\n",
    "SBM_CLASS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES=MODEL_TO_NUM_SAMPLES[SBM_CLASS]\n",
    "NUM_SAMPLES       # Trace sample length - If the dataset is built for phasenet: 3001 If it is for EQTransformer: 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAMPLE_RATE=100                                    # Sampling Rate - PhaseNet and EQTransformer expect 100Hz\n",
    "LARGE_ERROR_THRESHOLD_SECONDS=1                    # Onset prediction above this value (seconds) shall be considered large for metrics\n",
    "SYNTHESIZED_SNR_LIST=list(range(2,11))             # SNR levels of the synthetic data used`\n",
    "NUM_OF_ORIGINAL_TRACES = 2100                      # How many original traces to use for the noisy dataset - use slice from the start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LARGE_ERROR_THRESHOLD_SAMPLES=LARGE_ERROR_THRESHOLD_SECONDS*SAMPLE_RATE\n",
    "print(f'A residual of more than {LARGE_ERROR_THRESHOLD_SAMPLES} samples is considered large error')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SNR_CALC_STRATEGY_STR_ALTERNATIVES = ['energy_ratio', 'max_amplitude_vs_rms_ratio']\n",
    "SNR_CALC_STRATEGY_STR = 'energy_ratio'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Browse The path of the dataset the model is going to be evaluated on.\n",
    "The original data and synthetic noised data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET_PATH= os.path.join(DATASETS_ROOT_DIR, f'noisy_datasets/{dataset_origin}_{NUM_SAMPLES}_sample_joachim_noises_{SNR_CALC_STRATEGY_STR}_snr/')\n",
    "assert_path_exists(path_str=DATASET_PATH, name='DATASET_PATH')\n",
    "DATASET_PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NOISY_DATA_PATH_LIST = [os.path.join(DATASET_PATH, f'noisy_dataset_snr_{synthesized_snr}') for synthesized_snr in SYNTHESIZED_SNR_LIST ]\n",
    "assert_path_exists(path_str=DATASET_PATH, name='DATASET_PATH')\n",
    "for ndp in NOISY_DATA_PATH_LIST:\n",
    "    assert_path_exists(path_str=ndp, name='NOISY_DATA_PATH')\n",
    "NOISY_DATA_PATH_LIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Synthetic Noisy Traces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synthetic_noisy_dataset_paths= [os.path.join(ndp, 'traces.pt') for ndp in NOISY_DATA_PATH_LIST]\n",
    "\n",
    "synthetic_noisy_labels_paths= [os.path.join(ndp, 'labels.pt') for ndp in NOISY_DATA_PATH_LIST]\n",
    "\n",
    "augmented_noises_paths = [os.path.join(ndp, 'full_noise_traces.pt') for ndp in NOISY_DATA_PATH_LIST]\n",
    "\n",
    "factors_paths = [os.path.join(ndp, 'factors.pt') for ndp in NOISY_DATA_PATH_LIST]\n",
    "for synthetic_noisy_dataset_path, synthetic_noisy_labels_path, augmented_noises_path, factors_path in zip(synthetic_noisy_dataset_paths, synthetic_noisy_labels_paths,augmented_noises_paths, factors_paths):\n",
    "    assert_path_exists(path_str=synthetic_noisy_dataset_path, name='synthetic_noisy_dataset_path')\n",
    "    assert_path_exists(path_str=synthetic_noisy_labels_path, name='synthetic_noisy_labels_path')\n",
    "    assert_path_exists(path_str=augmented_noises_path, name='augmented_noises_path')\n",
    "    assert_path_exists(path_str=factors_path, name='factors_path')\n",
    "\n",
    "print('Synthetic Data will be stored in:')\n",
    "print(synthetic_noisy_dataset_paths)\n",
    "print(synthetic_noisy_labels_paths)\n",
    "print(augmented_noises_paths)\n",
    "print(factors_paths)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noisy_traces_list, noisy_traces_labels_list = [], []\n",
    "for synthetic_noisy_dataset_path, synthetic_noisy_labels_path in zip(synthetic_noisy_dataset_paths, synthetic_noisy_labels_paths):\n",
    "    synthetic_noisy_dataset, synthetic_noisy_labels = load_dataset_and_labels(dataset_path=synthetic_noisy_dataset_path, labels_path=synthetic_noisy_labels_path)\n",
    "    synthetic_noisy_dataset, synthetic_noisy_labels = synthetic_noisy_dataset.float(), synthetic_noisy_labels.float()\n",
    "\n",
    "    assert NUM_SAMPLES == synthetic_noisy_dataset.shape[-1], f'Expected Dataset contain {NUM_SAMPLES} samples. Got {synthetic_noisy_dataset.shape[-1]}'\n",
    "    assert synthetic_noisy_dataset.shape[0] == synthetic_noisy_labels.shape[0], f'Expected Dataset contain label for each trace. Got {synthetic_noisy_dataset.shape[0]} traces and {synthetic_noisy_labels.shape[0]} labels'\n",
    "\n",
    "    print(f'The loaded dataset has {synthetic_noisy_dataset.shape[0]} traces')\n",
    "    print(f'Each has {synthetic_noisy_dataset.shape[1]} channels of {synthetic_noisy_dataset.shape[2]} samples.')\n",
    "    print(f'Each entry is of type {synthetic_noisy_dataset.dtype}')\n",
    "\n",
    "    print(f'The loaded labels have {synthetic_noisy_labels.shape[0]} labels.')\n",
    "    print(f'Each entry is of type {synthetic_noisy_labels.dtype}')\n",
    "\n",
    "    noisy_traces_list.append(synthetic_noisy_dataset)\n",
    "    noisy_traces_labels_list.append(synthetic_noisy_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noisy_traces_list, noisy_traces_labels_list, total_indicies_to_use = remove_traces_not_to_use(noisy_traces_list=noisy_traces_list, noisy_labels_list=noisy_traces_labels_list, noisy_data_path_list=NOISY_DATA_PATH_LIST, num_of_original_traces=NUM_OF_ORIGINAL_TRACES)\n",
    "[(t.shape,l.shape) for (t,l) in zip(noisy_traces_list, noisy_traces_labels_list)], f'{len(total_indicies_to_use)} traces each snr level'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the original High SNR Traces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a dataset of high SNR traces taken from the original ETHZ\\GEOFON dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_traces_path = os.path.join(DATASET_PATH, 'original_dataset.pt')\n",
    "dataset_labels_path = os.path.join(DATASET_PATH, 'original_labels.pt')\n",
    "original_dataset = torch.load(dataset_traces_path)[total_indicies_to_use]  #[:num_traces]\n",
    "original_labels = torch.load(dataset_labels_path)[total_indicies_to_use]    #[:num_traces]\n",
    "\n",
    "num_original_traces = original_dataset.shape[0]\n",
    "num_original_labels = original_labels.shape[0]\n",
    "num_original_samples = original_dataset.shape[-1]\n",
    "\n",
    "assert num_original_labels == num_original_traces, f'Expected traces equal num labels.Got {num_original_traces} traces and {num_original_labels} labels'\n",
    "assert num_original_samples == NUM_SAMPLES, f'Expected {NUM_SAMPLES} in each trace. Got {num_original_samples}.'\n",
    "\n",
    "print(f'Loaded {num_original_traces} traces and corresponding labels.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load a Pretrained Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_model = load_pretrained_model(model_class=SBM_CLASS, dataset_trained_on=dataset_origin)\n",
    "pretrained_model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the Pretrained Model on  the Loaded Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Root-Mean-Squared-Errors (RMSE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The RMSE is the most common metric. Note that it is highly affected by outliars."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noisy_datasets_metric_result =[eval_mse(model = pretrained_model, traces=synthetic_noisy_dataset, labels=synthetic_noisy_labels,batch_size=32) for (synthetic_noisy_dataset, synthetic_noisy_labels) in zip(noisy_traces_list, noisy_traces_labels_list)]\n",
    "original_dataset_metric_result =  eval_mse(model = pretrained_model, traces=original_dataset, labels=original_labels,  batch_size=32)\n",
    "noisy_datasets_metric_result, original_dataset_metric_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noisy_datasets_metric_result_trimmed =[eval_mse(model = pretrained_model, traces=synthetic_noisy_dataset, labels=synthetic_noisy_labels, ignore_events_above_samples_threshold=500 ,batch_size=32) for (synthetic_noisy_dataset, synthetic_noisy_labels) in zip(noisy_traces_list, noisy_traces_labels_list)]\n",
    "original_dataset_metric_result_trimmed =  eval_mse(model = pretrained_model, traces=original_dataset, labels=original_labels, ignore_events_above_samples_threshold=500, batch_size=32)\n",
    "noisy_datasets_metric_result, original_dataset_metric_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax,ax_trimmed) = plt.subplots(1,2,figsize=(10,8), sharey='all' )\n",
    "plt.suptitle(f'RMSE(samples) vs. SNR(dB)  (dashed line - original data RMSE) SNR Estimated using {SNR_CALC_STRATEGY_STR}')\n",
    "ax.set_title('All residuals included')\n",
    "ax.plot(range(2,11),  noisy_datasets_metric_result);\n",
    "ax.hlines(y=original_dataset_metric_result, xmin=2, xmax=10, linestyles='dashed');\n",
    "ax_trimmed.set_title('Residuals more than 500 samples omitted')\n",
    "ax_trimmed.plot(range(2,11),  noisy_datasets_metric_result_trimmed);\n",
    "ax_trimmed.hlines(y=original_dataset_metric_result_trimmed, xmin=2, xmax=10, linestyles='dashed');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Large Errors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate both the original traces and the synthetic noisy traces and save the large error traces - the traces where the model had picking error (residual) larger than  predefined threshold (1 second by default - 100 samples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "large_error_traces_index_list_original_dataset = find_large_error_traces(dataset=original_dataset, model=pretrained_model.float(), labels=original_labels, threshold_samples=LARGE_ERROR_THRESHOLD_SAMPLES)\n",
    "\n",
    "\n",
    "large_error_traces_index_list_synthetic_noisy_datasets = [find_large_error_traces(dataset=synthetic_noisy_dataset, model=pretrained_model.float(), labels=synthetic_noisy_labels, threshold_samples=LARGE_ERROR_THRESHOLD_SAMPLES) for (synthetic_noisy_dataset, synthetic_noisy_labels) in zip(noisy_traces_list, noisy_traces_labels_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_lens = [int(len(large_error_traces_index_list_synthetic_noisy_dataset)) for large_error_traces_index_list_synthetic_noisy_dataset in large_error_traces_index_list_synthetic_noisy_datasets]\n",
    "\n",
    "print(f'There are {dataset_lens} large errors in the noisy datasets')\n",
    "\n",
    "dataset_len_original = int(len(large_error_traces_index_list_original_dataset))\n",
    "\n",
    "print(f'There are {dataset_len_original} large errors in the original dataset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "large_error_traces_index_list_original_dataset_trimmed = find_large_error_traces(dataset=original_dataset, model=pretrained_model.float(), labels=original_labels, threshold_samples=LARGE_ERROR_THRESHOLD_SAMPLES,  ignore_errors_larger_than_samples=500)\n",
    "\n",
    "\n",
    "large_error_traces_index_list_synthetic_noisy_datasets_trimmed = [find_large_error_traces(dataset=synthetic_noisy_dataset, model=pretrained_model.float(), labels=synthetic_noisy_labels, threshold_samples=LARGE_ERROR_THRESHOLD_SAMPLES, ignore_errors_larger_than_samples=500) for (synthetic_noisy_dataset, synthetic_noisy_labels) in zip(noisy_traces_list, noisy_traces_labels_list)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_trimmed_lens = [int(len(large_error_traces_index_list_synthetic_noisy_dataset_trimmed)) for large_error_traces_index_list_synthetic_noisy_dataset_trimmed in large_error_traces_index_list_synthetic_noisy_datasets_trimmed]\n",
    "\n",
    "print(f'There are {dataset_trimmed_lens} large errors in the noisy datasets')\n",
    "\n",
    "dataset_trimmed_len_original = int(len(large_error_traces_index_list_original_dataset_trimmed))\n",
    "\n",
    "print(f'There are {dataset_trimmed_len_original} large errors in the original dataset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax,ax_trimmed) = plt.subplots(1,2,figsize=(20,8), sharey='all')\n",
    "plt.suptitle(f'Large Error Count vs. SNR (dashed line - original data large error count) SNR Estimated using {SNR_CALC_STRATEGY_STR}')\n",
    "ax.set_title('All Errors Included')\n",
    "ax.plot(range(2,11),  dataset_lens);\n",
    "ax.hlines(y=dataset_len_original, xmin=2, xmax=10, linestyles='dashed');\n",
    "ax_trimmed.set_title('Errors Larger Than 500 samples Omitted')\n",
    "ax_trimmed.plot(range(2,11),  dataset_trimmed_lens);\n",
    "ax_trimmed.hlines(y=dataset_trimmed_len_original, xmin=2, xmax=10, linestyles='dashed');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maximum Value of Prediction Function vs. Residual for each SNR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aggregate all prediction functions maximum values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_values_list, residuals_list = [], []\n",
    "# Maximum values of th noisy traces predictions\n",
    "for noised_traces, noised_labels in zip(noisy_traces_list, noisy_traces_labels_list):\n",
    "    max_values, residuals = batch_max_values_and_residuals(batch=noised_traces, labels=noised_labels, model=pretrained_model)\n",
    "    max_values_list.append(max_values)\n",
    "    residuals_list.append(residuals)\n",
    "# Maximum values of the original traces\n",
    "orig_max_values, orig_residuals = batch_max_values_and_residuals(batch=original_dataset, labels=original_labels, model=pretrained_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot a scatter of All Max Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute The Mean and Standard Deviation of The Prediction Maximum Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "orig_max_values_mean = orig_max_values.mean()\n",
    "orig_max_values_std  = orig_max_values.std()\n",
    "orig_residuals_mean = orig_residuals.mean()\n",
    "orig_residuals_std = orig_residuals.std()\n",
    "\n",
    "orig_max_values.shape, orig_max_values_mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_values_mean_list = [max_values.mean() for max_values in max_values_list] + [orig_max_values_mean]\n",
    "max_values_stds_list = [max_values.std() for max_values in max_values_list] + [orig_max_values_std]\n",
    "residuals_means_list = [residuals.mean() for residuals in residuals_list] + [orig_residuals_mean]\n",
    "residuals_stds_list = [residuals.std() for residuals in residuals_list] + [orig_residuals_std]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20,6))\n",
    "plt.suptitle(f'Model Evaluation {str(SBM_CLASS)} on {str.upper(dataset_origin)}')\n",
    "# for i in range(4):\n",
    "x_value_list = list(range(2,11)) + ['orig']\n",
    "axs[0].set_title(f'Prediction Function Max Value mean')\n",
    "axs[0].plot(x_value_list, max_values_mean_list)\n",
    "axs[1].set_title(f'Prediction Function Max Value std')\n",
    "axs[1].plot(x_value_list, max_values_stds_list)\n",
    "axs[2].set_title(f'Residual mean')\n",
    "axs[2].plot(x_value_list, residuals_means_list)\n",
    "axs[3].set_title(f'Residuals std')\n",
    "axs[3].plot(x_value_list, residuals_stds_list)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Noised Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the `idx` variable to plot a different example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 7\n",
    "\n",
    "noised_traces =[synthetic_noisy_dataset[idx] for synthetic_noisy_dataset in noisy_traces_list]\n",
    "noised_labels =[synthetic_noisy_labels[idx] for synthetic_noisy_labels in noisy_traces_list]\n",
    "\n",
    "fig, axs = plt.subplots(10,1,figsize=(10,30), sharey='all')\n",
    "plt.suptitle(f'Synthetic Noised Versions Of the Same Original Trace - SNR Estimated using {SNR_CALC_STRATEGY_STR}')\n",
    "\n",
    "axs[0].set_title('Original Trace')\n",
    "axs[0].plot(original_dataset[idx,0])\n",
    "axs[0].vlines(x=original_labels[idx], ymin=-1, ymax=2, linestyles='dashed')\n",
    "for i in range(1,10):\n",
    "    trace = noised_traces[9-i][0]\n",
    "    axs[i].set_title(f'SNR {11 - i}')\n",
    "    axs[i].plot(trace)\n",
    "    axs[i].vlines(x=original_labels[idx], ymin=-1, ymax=2, linestyles='dashed')\n",
    "fig.subplots_adjust(hspace=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pretrained_model, noised_traces, noised_labels\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(10,1,figsize=(10,30), sharey='all')\n",
    "    plt.suptitle('Model Prediction Probability')\n",
    "    label = original_labels[idx]\n",
    "    pred_prob = pretrained_model(original_dataset[idx].unsqueeze(dim=0)).squeeze()\n",
    "    # pred_prob[:2,:(int(label)-500)] = 0\n",
    "    # pred_prob[:2,(int(label)+500):] = 0\n",
    "    axs[0].set_title(f'Original Trace')\n",
    "    axs[0].plot(pred_prob[0])\n",
    "    axs[0].vlines(x=original_labels[idx], ymin=-1, ymax=2, linestyles='dashed')\n",
    "    for i in range(1,10):\n",
    "        trace = noised_traces[9 - i]\n",
    "        trace = trace.unsqueeze(dim=0)\n",
    "        pred_prob = pretrained_model(trace).squeeze()\n",
    "        # pred_prob[:2,:(int(label)-500)] = 0\n",
    "        # pred_prob[:2,(int(label)+500):] = 0\n",
    "        axs[i].set_title(f'SNR {11 - i}')\n",
    "        axs[i].plot(pred_prob[0])\n",
    "        axs[i].vlines(x=original_labels[idx], ymin=-1, ymax=2, linestyles='dashed')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa76d2c83586c95486e2cc3c656ad2d093b47aefbf53fc633acb9a860f5157ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
